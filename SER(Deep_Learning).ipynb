
Speech Emotion Recognition using the RAVDESS dataset

Mounting Drive so that we can access the contents directly from the drive

In [ ]:
from google.colab import drive
drive.mount('/content/drive')
Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly

Enter your authorization code:
··········
Mounted at /content/drive
Importing Librosa library(A python package) for extracting important features of the audio like the pitch,tone.

In [ ]:
!pip install librosa
Requirement already satisfied: librosa in /usr/local/lib/python3.6/dist-packages (0.6.3)
Requirement already satisfied: six>=1.3 in /usr/local/lib/python3.6/dist-packages (from librosa) (1.12.0)
Requirement already satisfied: joblib>=0.12 in /usr/local/lib/python3.6/dist-packages (from librosa) (0.15.1)
Requirement already satisfied: numpy>=1.8.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (1.18.4)
Requirement already satisfied: numba>=0.38.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (0.48.0)
Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (1.4.1)
Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (0.22.2.post1)
Requirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (2.1.8)
Requirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (4.4.2)
Requirement already satisfied: resampy>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (0.2.2)
Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from numba>=0.38.0->librosa) (46.3.0)
Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /usr/local/lib/python3.6/dist-packages (from numba>=0.38.0->librosa) (0.31.0)
Loading one audio file from the drive using librosa. Librosa loads the audio file as a floating point time series.

In [ ]:
import librosa
from librosa import display

data, sampling_rate = librosa.load('/content/drive/My Drive/Ravtess/03-02-06-02-02-02-04.wav')
Plotting the audio file which was just loaded by librosa using waveplot function of librosa.This Function Plots the amplitude envelope of a waveform.

In [ ]:
% pylab inline
import matplotlib.pyplot as plt

plt.figure(figsize=(12, 4))
librosa.display.waveplot(data, sr=sampling_rate)
Populating the interactive namespace from numpy and matplotlib
/usr/local/lib/python3.6/dist-packages/IPython/core/magics/pylab.py:161: UserWarning: pylab import has clobbered these variables: ['display']
`%matplotlib` prevents importing * from pylab and numpy
  "\n`%matplotlib` prevents importing * from pylab and numpy"
Out[ ]:
<matplotlib.collections.PolyCollection at 0x7f9bfaa9dfd0>

Load all files
We will create our numpy array extracting Mel-frequency cepstral coefficients (MFCCs), while the classes to predict will be extracted from the name of the file (see the introductory section of this notebook to see the naming convention of the files of this dataset).

In [ ]:
import time
import os
path = '/content/drive/My Drive/Ravtess/'
lst = []

start_time = time.time()

for subdir, dirs, files in os.walk(path):
  for file in files:
      try:
        #Load librosa array, obtain mfcss, store the file and the mcss information in a new array
        X, sample_rate = librosa.load(os.path.join(subdir,file), res_type='kaiser_fast')
        mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T,axis=0) 
        # The instruction below converts the labels (from 1 to 8) to a series from 0 to 7
        # This is because our predictor needs to start from 0 otherwise it will try to predict also 0.
        file = int(file[7:8]) - 1 
        arr = mfccs, file
        lst.append(arr)
      # If the file is not valid, skip it
      except ValueError:
        continue

print("--- Data loaded. Loading time: %s seconds ---" % (time.time() - start_time))
--- Data loaded. Loading time: 2225.022324323654 seconds ---
In [ ]:
# Creating X and y: zip makes a list of all the first elements, and a list of all the second elements.
X, y = zip(*lst)
In [ ]:
import numpy as np
X = np.asarray(X)
y = np.asarray(y)


X.shape, y.shape
Out[ ]:
((5252, 40), (5252,))
In [ ]:
# Saving joblib files to not load them again with the loop above

import joblib

X_name = 'X.joblib'
y_name = 'y.joblib'
save_dir = '/content/drive/My Drive/Ravtess_model'

savedX = joblib.dump(X, os.path.join(save_dir, X_name))
savedy = joblib.dump(y, os.path.join(save_dir, y_name))
In [ ]:
# Loading saved models
import joblib
X = joblib.load('/content/drive/My Drive/Ravtess_model/X.joblib')
y = joblib.load('/content/drive/My Drive/Ravtess_model/y.joblib')
Decision Tree Classifier
To make a first attempt in accomplishing this classification task I chose a decision tree:

In [ ]:
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.33, random_state=42)
In [ ]:
from sklearn.tree import DecisionTreeClassifier
In [ ]:
dtree = DecisionTreeClassifier()
In [ ]:
dtree.fit(X_train, y_train)
Out[ ]:
DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_impurity_split=None,
                       min_samples_leaf=1, min_samples_split=2,
                       min_weight_fraction_leaf=0.0, presort='deprecated',
                       random_state=None, splitter='best')
In [ ]:
predictions = dtree.predict(X_test)
In [ ]:
from sklearn.metrics import classification_report,confusion_matrix
print(classification_report(y_test,predictions))
              precision    recall  f1-score   support

           0       0.82      0.79      0.80       190
           1       0.61      0.54      0.57       117
           2       0.65      0.65      0.65       266
           3       0.72      0.75      0.73       246
           4       0.71      0.73      0.72       265
           5       0.64      0.65      0.65       246
           6       0.59      0.66      0.62       202
           7       0.69      0.60      0.64       202

    accuracy                           0.68      1734
   macro avg       0.68      0.67      0.67      1734
weighted avg       0.68      0.68      0.68      1734

Neural network
In [ ]:
import numpy as np
x_traincnn = np.expand_dims(X_train, axis=2)
x_testcnn = np.expand_dims(X_test, axis=2)
In [ ]:
x_traincnn.shape, x_testcnn.shape
Out[ ]:
((3518, 40, 1), (1734, 40, 1))
In [ ]:
import keras
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from keras.preprocessing import sequence
from keras.models import Sequential
from keras.layers import Dense, Embedding
from keras.utils import to_categorical
from keras.layers import Input, Flatten, Dropout, Activation
from keras.layers import Conv1D, MaxPooling1D
from keras.models import Model
from keras.callbacks import ModelCheckpoint

model = Sequential()

model.add(Conv1D(64, 5,padding='same',
                 input_shape=(40,1)))
model.add(Activation('relu'))
model.add(Dropout(0.1))
model.add(MaxPooling1D(pool_size=(4)))
model.add(Conv1D(128, 5,padding='same',))
model.add(Activation('relu'))
model.add(Dropout(0.1))
model.add(MaxPooling1D(pool_size=(4)))
model.add(Conv1D(256, 5,padding='same',))
model.add(Activation('relu'))
model.add(Dropout(0.1))
model.add(Flatten())
model.add(Dense(8))
model.add(Activation('softmax'))
opt = keras.optimizers.rmsprop(lr=0.00005, rho=0.9, epsilon=1e-07, decay=0.0)
Using TensorFlow backend.
In [ ]:
model.summary()
Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv1d_1 (Conv1D)            (None, 40, 64)            384       
_________________________________________________________________
activation_1 (Activation)    (None, 40, 64)            0         
_________________________________________________________________
dropout_1 (Dropout)          (None, 40, 64)            0         
_________________________________________________________________
max_pooling1d_1 (MaxPooling1 (None, 10, 64)            0         
_________________________________________________________________
conv1d_2 (Conv1D)            (None, 10, 128)           41088     
_________________________________________________________________
activation_2 (Activation)    (None, 10, 128)           0         
_________________________________________________________________
dropout_2 (Dropout)          (None, 10, 128)           0         
_________________________________________________________________
max_pooling1d_2 (MaxPooling1 (None, 2, 128)            0         
_________________________________________________________________
conv1d_3 (Conv1D)            (None, 2, 256)            164096    
_________________________________________________________________
activation_3 (Activation)    (None, 2, 256)            0         
_________________________________________________________________
dropout_3 (Dropout)          (None, 2, 256)            0         
_________________________________________________________________
flatten_1 (Flatten)          (None, 512)               0         
_________________________________________________________________
dense_1 (Dense)              (None, 8)                 4104      
_________________________________________________________________
activation_4 (Activation)    (None, 8)                 0         
=================================================================
Total params: 209,672
Trainable params: 209,672
Non-trainable params: 0
_________________________________________________________________
In [ ]:
model.compile(loss='sparse_categorical_crossentropy',
              optimizer=opt,
              metrics=['accuracy'])
In [ ]:
cnnhistory=model.fit(x_traincnn, y_train, batch_size=16, epochs=200, validation_data=(x_testcnn, y_test))
Train on 3518 samples, validate on 1734 samples
Epoch 1/200
3518/3518 [==============================] - 4s 1ms/step - loss: 3.4102 - accuracy: 0.1848 - val_loss: 1.8985 - val_accuracy: 0.3610
Epoch 2/200
3518/3518 [==============================] - 4s 1ms/step - loss: 2.2503 - accuracy: 0.3110 - val_loss: 1.4561 - val_accuracy: 0.5127
Epoch 3/200
3518/3518 [==============================] - 4s 1ms/step - loss: 1.8465 - accuracy: 0.4073 - val_loss: 1.4126 - val_accuracy: 0.5283
Epoch 4/200
3518/3518 [==============================] - 4s 1ms/step - loss: 1.6013 - accuracy: 0.4687 - val_loss: 1.2565 - val_accuracy: 0.5577
Epoch 5/200
3518/3518 [==============================] - 4s 1ms/step - loss: 1.4437 - accuracy: 0.5122 - val_loss: 1.2132 - val_accuracy: 0.5686
Epoch 6/200
3518/3518 [==============================] - 4s 1ms/step - loss: 1.3279 - accuracy: 0.5389 - val_loss: 1.1017 - val_accuracy: 0.6136
Epoch 7/200
3518/3518 [==============================] - 4s 1ms/step - loss: 1.2669 - accuracy: 0.5620 - val_loss: 1.0511 - val_accuracy: 0.6557
Epoch 8/200
3518/3518 [==============================] - 4s 1ms/step - loss: 1.1955 - accuracy: 0.5790 - val_loss: 0.9943 - val_accuracy: 0.6494
Epoch 9/200
3518/3518 [==============================] - 4s 1ms/step - loss: 1.1468 - accuracy: 0.5893 - val_loss: 0.9513 - val_accuracy: 0.6736
Epoch 10/200
3518/3518 [==============================] - 4s 1ms/step - loss: 1.0948 - accuracy: 0.6174 - val_loss: 0.9724 - val_accuracy: 0.6482
Epoch 11/200
3518/3518 [==============================] - 4s 1ms/step - loss: 1.0643 - accuracy: 0.6248 - val_loss: 0.9280 - val_accuracy: 0.6551
Epoch 12/200
3518/3518 [==============================] - 4s 1ms/step - loss: 1.0331 - accuracy: 0.6293 - val_loss: 0.8843 - val_accuracy: 0.6880
Epoch 13/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.9942 - accuracy: 0.6342 - val_loss: 0.8565 - val_accuracy: 0.7070
Epoch 14/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.9810 - accuracy: 0.6396 - val_loss: 0.8485 - val_accuracy: 0.6932
Epoch 15/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.9676 - accuracy: 0.6470 - val_loss: 0.8289 - val_accuracy: 0.7134
Epoch 16/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.9260 - accuracy: 0.6592 - val_loss: 0.8168 - val_accuracy: 0.7042
Epoch 17/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.9187 - accuracy: 0.6677 - val_loss: 0.8041 - val_accuracy: 0.7301
Epoch 18/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.8856 - accuracy: 0.6691 - val_loss: 0.7983 - val_accuracy: 0.7030
Epoch 19/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.8818 - accuracy: 0.6734 - val_loss: 0.7894 - val_accuracy: 0.7128
Epoch 20/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.8681 - accuracy: 0.6762 - val_loss: 0.7678 - val_accuracy: 0.7232
Epoch 21/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.8524 - accuracy: 0.6828 - val_loss: 0.7858 - val_accuracy: 0.7030
Epoch 22/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.8458 - accuracy: 0.6805 - val_loss: 0.7415 - val_accuracy: 0.7284
Epoch 23/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.8329 - accuracy: 0.6825 - val_loss: 0.7353 - val_accuracy: 0.7445
Epoch 24/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.8061 - accuracy: 0.7007 - val_loss: 0.7341 - val_accuracy: 0.7341
Epoch 25/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.7971 - accuracy: 0.7055 - val_loss: 0.7264 - val_accuracy: 0.7295
Epoch 26/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.7879 - accuracy: 0.7041 - val_loss: 0.6922 - val_accuracy: 0.7595
Epoch 27/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.7802 - accuracy: 0.7138 - val_loss: 0.6993 - val_accuracy: 0.7509
Epoch 28/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.7555 - accuracy: 0.7189 - val_loss: 0.7035 - val_accuracy: 0.7382
Epoch 29/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.7548 - accuracy: 0.7223 - val_loss: 0.7083 - val_accuracy: 0.7370
Epoch 30/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.7606 - accuracy: 0.7126 - val_loss: 0.6965 - val_accuracy: 0.7434
Epoch 31/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.7404 - accuracy: 0.7223 - val_loss: 0.6665 - val_accuracy: 0.7682
Epoch 32/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.7357 - accuracy: 0.7288 - val_loss: 0.6739 - val_accuracy: 0.7532
Epoch 33/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.7282 - accuracy: 0.7280 - val_loss: 0.6587 - val_accuracy: 0.7555
Epoch 34/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.7206 - accuracy: 0.7302 - val_loss: 0.6559 - val_accuracy: 0.7647
Epoch 35/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.7125 - accuracy: 0.7402 - val_loss: 0.6419 - val_accuracy: 0.7705
Epoch 36/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.7025 - accuracy: 0.7425 - val_loss: 0.6475 - val_accuracy: 0.7589
Epoch 37/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.7023 - accuracy: 0.7447 - val_loss: 0.6266 - val_accuracy: 0.7641
Epoch 38/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.6784 - accuracy: 0.7402 - val_loss: 0.6454 - val_accuracy: 0.7670
Epoch 39/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.6800 - accuracy: 0.7453 - val_loss: 0.6645 - val_accuracy: 0.7468
Epoch 40/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.6834 - accuracy: 0.7493 - val_loss: 0.6199 - val_accuracy: 0.7797
Epoch 41/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.6686 - accuracy: 0.7538 - val_loss: 0.6227 - val_accuracy: 0.7722
Epoch 42/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.6564 - accuracy: 0.7609 - val_loss: 0.6068 - val_accuracy: 0.7837
Epoch 43/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.6637 - accuracy: 0.7541 - val_loss: 0.6003 - val_accuracy: 0.7849
Epoch 44/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.6544 - accuracy: 0.7544 - val_loss: 0.6117 - val_accuracy: 0.7722
Epoch 45/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.6467 - accuracy: 0.7530 - val_loss: 0.6069 - val_accuracy: 0.7739
Epoch 46/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.6390 - accuracy: 0.7590 - val_loss: 0.5959 - val_accuracy: 0.7791
Epoch 47/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.6398 - accuracy: 0.7612 - val_loss: 0.6056 - val_accuracy: 0.7832
Epoch 48/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.6341 - accuracy: 0.7607 - val_loss: 0.5879 - val_accuracy: 0.7953
Epoch 49/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.6302 - accuracy: 0.7689 - val_loss: 0.5790 - val_accuracy: 0.7907
Epoch 50/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.6185 - accuracy: 0.7638 - val_loss: 0.5820 - val_accuracy: 0.7935
Epoch 51/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.6044 - accuracy: 0.7760 - val_loss: 0.5716 - val_accuracy: 0.7947
Epoch 52/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.6106 - accuracy: 0.7715 - val_loss: 0.5917 - val_accuracy: 0.7809
Epoch 53/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.6096 - accuracy: 0.7749 - val_loss: 0.5686 - val_accuracy: 0.8022
Epoch 54/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.6032 - accuracy: 0.7763 - val_loss: 0.5641 - val_accuracy: 0.7964
Epoch 55/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.5913 - accuracy: 0.7825 - val_loss: 0.5700 - val_accuracy: 0.7941
Epoch 56/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.5896 - accuracy: 0.7749 - val_loss: 0.5676 - val_accuracy: 0.7976
Epoch 57/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.5899 - accuracy: 0.7845 - val_loss: 0.5647 - val_accuracy: 0.7976
Epoch 58/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.5824 - accuracy: 0.7789 - val_loss: 0.5538 - val_accuracy: 0.8039
Epoch 59/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.5788 - accuracy: 0.7823 - val_loss: 0.5508 - val_accuracy: 0.8033
Epoch 60/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.5731 - accuracy: 0.7905 - val_loss: 0.5589 - val_accuracy: 0.7970
Epoch 61/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.5724 - accuracy: 0.7894 - val_loss: 0.5633 - val_accuracy: 0.7895
Epoch 62/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.5706 - accuracy: 0.7897 - val_loss: 0.5441 - val_accuracy: 0.7999
Epoch 63/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.5615 - accuracy: 0.7860 - val_loss: 0.5564 - val_accuracy: 0.7982
Epoch 64/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.5529 - accuracy: 0.7985 - val_loss: 0.5502 - val_accuracy: 0.8028
Epoch 65/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.5601 - accuracy: 0.7899 - val_loss: 0.5589 - val_accuracy: 0.7987
Epoch 66/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.5587 - accuracy: 0.8005 - val_loss: 0.5307 - val_accuracy: 0.8108
Epoch 67/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.5465 - accuracy: 0.8024 - val_loss: 0.5296 - val_accuracy: 0.8091
Epoch 68/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.5443 - accuracy: 0.7959 - val_loss: 0.5362 - val_accuracy: 0.8068
Epoch 69/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.5548 - accuracy: 0.7905 - val_loss: 0.5435 - val_accuracy: 0.8033
Epoch 70/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.5326 - accuracy: 0.8064 - val_loss: 0.5240 - val_accuracy: 0.8108
Epoch 71/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.5263 - accuracy: 0.8098 - val_loss: 0.5403 - val_accuracy: 0.8080
Epoch 72/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.5303 - accuracy: 0.8050 - val_loss: 0.5155 - val_accuracy: 0.8160
Epoch 73/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.5241 - accuracy: 0.8076 - val_loss: 0.5253 - val_accuracy: 0.8114
Epoch 74/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.5240 - accuracy: 0.8056 - val_loss: 0.5219 - val_accuracy: 0.8149
Epoch 75/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.5289 - accuracy: 0.8064 - val_loss: 0.5148 - val_accuracy: 0.8189
Epoch 76/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.5111 - accuracy: 0.8144 - val_loss: 0.5246 - val_accuracy: 0.8016
Epoch 77/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.5016 - accuracy: 0.8181 - val_loss: 0.5139 - val_accuracy: 0.8103
Epoch 78/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.5130 - accuracy: 0.8167 - val_loss: 0.5236 - val_accuracy: 0.8120
Epoch 79/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.5182 - accuracy: 0.8096 - val_loss: 0.5174 - val_accuracy: 0.8131
Epoch 80/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.5006 - accuracy: 0.8127 - val_loss: 0.5004 - val_accuracy: 0.8258
Epoch 81/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.4993 - accuracy: 0.8186 - val_loss: 0.5141 - val_accuracy: 0.8137
Epoch 82/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.5037 - accuracy: 0.8141 - val_loss: 0.5027 - val_accuracy: 0.8195
Epoch 83/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.4926 - accuracy: 0.8206 - val_loss: 0.5014 - val_accuracy: 0.8189
Epoch 84/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.4946 - accuracy: 0.8147 - val_loss: 0.5052 - val_accuracy: 0.8247
Epoch 85/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.4841 - accuracy: 0.8221 - val_loss: 0.4867 - val_accuracy: 0.8264
Epoch 86/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.4933 - accuracy: 0.8186 - val_loss: 0.5022 - val_accuracy: 0.8149
Epoch 87/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.4874 - accuracy: 0.8260 - val_loss: 0.5016 - val_accuracy: 0.8206
Epoch 88/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.4917 - accuracy: 0.8221 - val_loss: 0.5036 - val_accuracy: 0.8178
Epoch 89/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.4796 - accuracy: 0.8252 - val_loss: 0.5007 - val_accuracy: 0.8230
Epoch 90/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.4715 - accuracy: 0.8246 - val_loss: 0.5000 - val_accuracy: 0.8183
Epoch 91/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.4755 - accuracy: 0.8238 - val_loss: 0.5266 - val_accuracy: 0.8155
Epoch 92/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.4683 - accuracy: 0.8258 - val_loss: 0.5068 - val_accuracy: 0.8172
Epoch 93/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.4599 - accuracy: 0.8275 - val_loss: 0.4872 - val_accuracy: 0.8287
Epoch 94/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.4646 - accuracy: 0.8283 - val_loss: 0.4990 - val_accuracy: 0.8189
Epoch 95/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.4680 - accuracy: 0.8272 - val_loss: 0.4860 - val_accuracy: 0.8316
Epoch 96/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.4769 - accuracy: 0.8223 - val_loss: 0.4941 - val_accuracy: 0.8264
Epoch 97/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.4570 - accuracy: 0.8312 - val_loss: 0.4776 - val_accuracy: 0.8328
Epoch 98/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.4599 - accuracy: 0.8380 - val_loss: 0.4913 - val_accuracy: 0.8131
Epoch 99/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.4633 - accuracy: 0.8289 - val_loss: 0.4918 - val_accuracy: 0.8189
Epoch 100/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.4471 - accuracy: 0.8366 - val_loss: 0.4819 - val_accuracy: 0.8281
Epoch 101/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.4441 - accuracy: 0.8354 - val_loss: 0.4764 - val_accuracy: 0.8281
Epoch 102/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.4505 - accuracy: 0.8354 - val_loss: 0.5071 - val_accuracy: 0.8195
Epoch 103/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.4502 - accuracy: 0.8363 - val_loss: 0.4857 - val_accuracy: 0.8195
Epoch 104/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.4349 - accuracy: 0.8351 - val_loss: 0.4737 - val_accuracy: 0.8322
Epoch 105/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.4415 - accuracy: 0.8371 - val_loss: 0.4777 - val_accuracy: 0.8287
Epoch 106/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.4374 - accuracy: 0.8403 - val_loss: 0.4859 - val_accuracy: 0.8264
Epoch 107/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.4412 - accuracy: 0.8411 - val_loss: 0.4696 - val_accuracy: 0.8299
Epoch 108/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.4279 - accuracy: 0.8454 - val_loss: 0.4790 - val_accuracy: 0.8206
Epoch 109/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.4375 - accuracy: 0.8425 - val_loss: 0.4775 - val_accuracy: 0.8322
Epoch 110/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.4342 - accuracy: 0.8403 - val_loss: 0.4614 - val_accuracy: 0.8385
Epoch 111/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.4262 - accuracy: 0.8457 - val_loss: 0.4790 - val_accuracy: 0.8328
Epoch 112/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.4268 - accuracy: 0.8422 - val_loss: 0.4630 - val_accuracy: 0.8374
Epoch 113/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.4208 - accuracy: 0.8445 - val_loss: 0.4802 - val_accuracy: 0.8287
Epoch 114/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.4208 - accuracy: 0.8448 - val_loss: 0.4642 - val_accuracy: 0.8374
Epoch 115/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.4168 - accuracy: 0.8485 - val_loss: 0.4644 - val_accuracy: 0.8304
Epoch 116/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.4156 - accuracy: 0.8488 - val_loss: 0.4561 - val_accuracy: 0.8385
Epoch 117/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.4078 - accuracy: 0.8502 - val_loss: 0.4610 - val_accuracy: 0.8356
Epoch 118/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.4122 - accuracy: 0.8442 - val_loss: 0.4496 - val_accuracy: 0.8374
Epoch 119/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.4090 - accuracy: 0.8493 - val_loss: 0.4646 - val_accuracy: 0.8374
Epoch 120/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.4095 - accuracy: 0.8491 - val_loss: 0.4738 - val_accuracy: 0.8304
Epoch 121/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.4043 - accuracy: 0.8530 - val_loss: 0.4555 - val_accuracy: 0.8351
Epoch 122/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.3968 - accuracy: 0.8550 - val_loss: 0.4541 - val_accuracy: 0.8362
Epoch 123/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.4107 - accuracy: 0.8545 - val_loss: 0.4551 - val_accuracy: 0.8374
Epoch 124/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.3967 - accuracy: 0.8556 - val_loss: 0.4570 - val_accuracy: 0.8397
Epoch 125/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.4031 - accuracy: 0.8519 - val_loss: 0.4481 - val_accuracy: 0.8391
Epoch 126/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.3949 - accuracy: 0.8493 - val_loss: 0.4706 - val_accuracy: 0.8258
Epoch 127/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.3945 - accuracy: 0.8587 - val_loss: 0.4469 - val_accuracy: 0.8379
Epoch 128/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.3901 - accuracy: 0.8556 - val_loss: 0.4468 - val_accuracy: 0.8403
Epoch 129/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.3860 - accuracy: 0.8556 - val_loss: 0.4567 - val_accuracy: 0.8379
Epoch 130/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.3818 - accuracy: 0.8627 - val_loss: 0.4394 - val_accuracy: 0.8426
Epoch 131/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.3780 - accuracy: 0.8627 - val_loss: 0.4432 - val_accuracy: 0.8431
Epoch 132/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.3871 - accuracy: 0.8653 - val_loss: 0.4569 - val_accuracy: 0.8339
Epoch 133/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.3743 - accuracy: 0.8627 - val_loss: 0.4695 - val_accuracy: 0.8339
Epoch 134/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.3753 - accuracy: 0.8590 - val_loss: 0.4476 - val_accuracy: 0.8420
Epoch 135/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.3784 - accuracy: 0.8624 - val_loss: 0.4501 - val_accuracy: 0.8414
Epoch 136/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.3757 - accuracy: 0.8616 - val_loss: 0.4565 - val_accuracy: 0.8368
Epoch 137/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.3678 - accuracy: 0.8664 - val_loss: 0.4477 - val_accuracy: 0.8437
Epoch 138/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.3658 - accuracy: 0.8678 - val_loss: 0.4342 - val_accuracy: 0.8518
Epoch 139/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.3620 - accuracy: 0.8698 - val_loss: 0.4455 - val_accuracy: 0.8379
Epoch 140/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.3663 - accuracy: 0.8624 - val_loss: 0.4600 - val_accuracy: 0.8374
Epoch 141/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.3667 - accuracy: 0.8616 - val_loss: 0.4502 - val_accuracy: 0.8437
Epoch 142/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.3628 - accuracy: 0.8644 - val_loss: 0.4544 - val_accuracy: 0.8431
Epoch 143/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.3580 - accuracy: 0.8692 - val_loss: 0.4386 - val_accuracy: 0.8403
Epoch 144/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.3646 - accuracy: 0.8707 - val_loss: 0.4627 - val_accuracy: 0.8356
Epoch 145/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.3609 - accuracy: 0.8704 - val_loss: 0.4398 - val_accuracy: 0.8472
Epoch 146/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.3447 - accuracy: 0.8764 - val_loss: 0.4363 - val_accuracy: 0.8443
Epoch 147/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.3465 - accuracy: 0.8752 - val_loss: 0.4791 - val_accuracy: 0.8351
Epoch 148/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.3465 - accuracy: 0.8687 - val_loss: 0.4405 - val_accuracy: 0.8420
Epoch 149/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.3507 - accuracy: 0.8707 - val_loss: 0.4261 - val_accuracy: 0.8483
Epoch 150/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.3505 - accuracy: 0.8735 - val_loss: 0.4305 - val_accuracy: 0.8489
Epoch 151/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.3646 - accuracy: 0.8638 - val_loss: 0.4307 - val_accuracy: 0.8518
Epoch 152/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.3462 - accuracy: 0.8732 - val_loss: 0.4337 - val_accuracy: 0.8512
Epoch 153/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.3456 - accuracy: 0.8764 - val_loss: 0.4360 - val_accuracy: 0.8483
Epoch 154/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.3413 - accuracy: 0.8758 - val_loss: 0.4445 - val_accuracy: 0.8374
Epoch 155/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.3440 - accuracy: 0.8732 - val_loss: 0.4439 - val_accuracy: 0.8443
Epoch 156/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.3484 - accuracy: 0.8755 - val_loss: 0.4424 - val_accuracy: 0.8420
Epoch 157/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.3335 - accuracy: 0.8778 - val_loss: 0.4370 - val_accuracy: 0.8420
Epoch 158/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.3349 - accuracy: 0.8721 - val_loss: 0.4405 - val_accuracy: 0.8454
Epoch 159/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.3411 - accuracy: 0.8778 - val_loss: 0.4329 - val_accuracy: 0.8524
Epoch 160/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.3449 - accuracy: 0.8712 - val_loss: 0.4312 - val_accuracy: 0.8489
Epoch 161/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.3237 - accuracy: 0.8803 - val_loss: 0.4586 - val_accuracy: 0.8414
Epoch 162/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.3238 - accuracy: 0.8869 - val_loss: 0.4542 - val_accuracy: 0.8437
Epoch 163/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.3278 - accuracy: 0.8815 - val_loss: 0.4601 - val_accuracy: 0.8374
Epoch 164/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.3182 - accuracy: 0.8829 - val_loss: 0.4326 - val_accuracy: 0.8483
Epoch 165/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.3238 - accuracy: 0.8843 - val_loss: 0.4450 - val_accuracy: 0.8489
Epoch 166/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.3177 - accuracy: 0.8846 - val_loss: 0.4299 - val_accuracy: 0.8501
Epoch 167/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.3154 - accuracy: 0.8812 - val_loss: 0.4337 - val_accuracy: 0.8535
Epoch 168/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.3290 - accuracy: 0.8755 - val_loss: 0.4335 - val_accuracy: 0.8495
Epoch 169/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.3111 - accuracy: 0.8832 - val_loss: 0.4265 - val_accuracy: 0.8483
Epoch 170/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.3158 - accuracy: 0.8846 - val_loss: 0.4337 - val_accuracy: 0.8472
Epoch 171/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.3186 - accuracy: 0.8843 - val_loss: 0.4444 - val_accuracy: 0.8483
Epoch 172/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.3009 - accuracy: 0.8846 - val_loss: 0.4530 - val_accuracy: 0.8489
Epoch 173/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.3104 - accuracy: 0.8900 - val_loss: 0.4394 - val_accuracy: 0.8524
Epoch 174/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.3043 - accuracy: 0.8835 - val_loss: 0.4289 - val_accuracy: 0.8472
Epoch 175/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.3058 - accuracy: 0.8894 - val_loss: 0.4270 - val_accuracy: 0.8524
Epoch 176/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.3173 - accuracy: 0.8849 - val_loss: 0.4403 - val_accuracy: 0.8512
Epoch 177/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.2994 - accuracy: 0.8960 - val_loss: 0.4321 - val_accuracy: 0.8489
Epoch 178/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.3090 - accuracy: 0.8860 - val_loss: 0.4270 - val_accuracy: 0.8587
Epoch 179/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.2966 - accuracy: 0.8914 - val_loss: 0.4244 - val_accuracy: 0.8518
Epoch 180/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.2960 - accuracy: 0.8957 - val_loss: 0.4342 - val_accuracy: 0.8501
Epoch 181/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.3101 - accuracy: 0.8857 - val_loss: 0.4191 - val_accuracy: 0.8587
Epoch 182/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.3140 - accuracy: 0.8854 - val_loss: 0.4264 - val_accuracy: 0.8564
Epoch 183/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.2922 - accuracy: 0.8985 - val_loss: 0.4370 - val_accuracy: 0.8466
Epoch 184/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.2985 - accuracy: 0.8880 - val_loss: 0.4268 - val_accuracy: 0.8518
Epoch 185/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.2892 - accuracy: 0.8883 - val_loss: 0.4331 - val_accuracy: 0.8483
Epoch 186/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.2945 - accuracy: 0.8960 - val_loss: 0.4395 - val_accuracy: 0.8495
Epoch 187/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.2950 - accuracy: 0.8928 - val_loss: 0.4224 - val_accuracy: 0.8558
Epoch 188/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.2939 - accuracy: 0.8937 - val_loss: 0.4268 - val_accuracy: 0.8501
Epoch 189/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.2886 - accuracy: 0.8940 - val_loss: 0.4284 - val_accuracy: 0.8576
Epoch 190/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.2932 - accuracy: 0.8948 - val_loss: 0.4224 - val_accuracy: 0.8558
Epoch 191/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.2902 - accuracy: 0.8937 - val_loss: 0.4295 - val_accuracy: 0.8581
Epoch 192/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.2963 - accuracy: 0.8900 - val_loss: 0.4200 - val_accuracy: 0.8518
Epoch 193/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.2838 - accuracy: 0.8960 - val_loss: 0.4124 - val_accuracy: 0.8547
Epoch 194/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.2789 - accuracy: 0.8985 - val_loss: 0.4200 - val_accuracy: 0.8466
Epoch 195/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.2860 - accuracy: 0.8937 - val_loss: 0.4145 - val_accuracy: 0.8535
Epoch 196/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.2770 - accuracy: 0.9008 - val_loss: 0.4315 - val_accuracy: 0.8512
Epoch 197/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.2808 - accuracy: 0.8962 - val_loss: 0.4285 - val_accuracy: 0.8547
Epoch 198/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.2751 - accuracy: 0.8988 - val_loss: 0.4522 - val_accuracy: 0.8495
Epoch 199/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.2762 - accuracy: 0.8997 - val_loss: 0.4226 - val_accuracy: 0.8501
Epoch 200/200
3518/3518 [==============================] - 4s 1ms/step - loss: 0.2579 - accuracy: 0.9039 - val_loss: 0.4277 - val_accuracy: 0.8547
In [ ]:
plt.plot(cnnhistory.history['loss'])
plt.plot(cnnhistory.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

In [ ]:
plt.plot(cnnhistory.history['accuracy'])
plt.plot(cnnhistory.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

In [ ]:
predictions = model.predict_classes(x_testcnn)
In [ ]:
predictions
Out[ ]:
array([3, 4, 3, ..., 2, 3, 5])
In [ ]:
y_test
Out[ ]:
array([3, 4, 3, ..., 2, 3, 5])
In [ ]:
new_Ytest = y_test.astype(int)
In [ ]:
new_Ytest
Out[ ]:
array([3, 4, 3, ..., 2, 3, 5])
In [ ]:
from sklearn.metrics import classification_report
report = classification_report(new_Ytest, predictions)
print(report)
              precision    recall  f1-score   support

           0       0.88      0.91      0.89       190
           1       0.77      0.76      0.76       117
           2       0.90      0.84      0.87       266
           3       0.80      0.86      0.83       246
           4       0.89      0.88      0.88       265
           5       0.88      0.80      0.84       246
           6       0.81      0.92      0.86       202
           7       0.88      0.85      0.86       202

    accuracy                           0.85      1734
   macro avg       0.85      0.85      0.85      1734
weighted avg       0.86      0.85      0.85      1734

In [ ]:
from sklearn.metrics import confusion_matrix
matrix = confusion_matrix(new_Ytest, predictions)
print (matrix)

# 0 = neutral, 1 = calm, 2 = happy, 3 = sad, 4 = angry, 5 = fearful, 6 = disgust, 7 = surprised
[[172   5   1   8   0   0   1   3]
 [  6  89   5   9   0   0   6   2]
 [  4   6 223   5   8   8   3   9]
 [  7   7   3 212   3   8   4   2]
 [  1   2   4   2 232   8  15   1]
 [  0   1   6  23  10 196   8   2]
 [  1   2   0   2   5   1 186   5]
 [  5   4   6   4   2   1   8 172]]
In [ ]:
model.save('testing10_model.h5')
print("MODEL SAVED")
MODEL SAVED
In [ ]:
new_model=keras.models.load_model('testing10_model.h5')
new_model.summary()
Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv1d_1 (Conv1D)            (None, 40, 64)            384       
_________________________________________________________________
activation_1 (Activation)    (None, 40, 64)            0         
_________________________________________________________________
dropout_1 (Dropout)          (None, 40, 64)            0         
_________________________________________________________________
max_pooling1d_1 (MaxPooling1 (None, 10, 64)            0         
_________________________________________________________________
conv1d_2 (Conv1D)            (None, 10, 128)           41088     
_________________________________________________________________
activation_2 (Activation)    (None, 10, 128)           0         
_________________________________________________________________
dropout_2 (Dropout)          (None, 10, 128)           0         
_________________________________________________________________
max_pooling1d_2 (MaxPooling1 (None, 2, 128)            0         
_________________________________________________________________
conv1d_3 (Conv1D)            (None, 2, 256)            164096    
_________________________________________________________________
activation_3 (Activation)    (None, 2, 256)            0         
_________________________________________________________________
dropout_3 (Dropout)          (None, 2, 256)            0         
_________________________________________________________________
flatten_1 (Flatten)          (None, 512)               0         
_________________________________________________________________
dense_1 (Dense)              (None, 8)                 4104      
_________________________________________________________________
activation_4 (Activation)    (None, 8)                 0         
=================================================================
Total params: 209,672
Trainable params: 209,672
Non-trainable params: 0
_________________________________________________________________
In [ ]:
loss, acc = new_model.evaluate(x_testcnn, y_test)
print("Restored model, accuracy: {:5.2f}%".format(100*acc))
1734/1734 [==============================] - 0s 95us/step
Restored model, accuracy: 85.47%
In [ ]:
